[package]
name = "llm-serving"
version = "0.1.0"
edition = "2024"

[dependencies]
axum = { version = "0.7", features = ["macros"] }
tokio = { version = "1.35", features = ["full", "sync"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tracing = "0.1"
llama_cpp = { version = "0.3.2", optional = true }
uuid = { version = "1.0", features = ["v4"] }
tokio-stream = "0.1"
async-trait = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt"] }
futures = "0.3"
tower = "0.5"
moka = { version = "0.12", features = ["future"] }
sha2 = "0.10"
memmap2 = "0.9"
rand = "0.8"
metrics = "0.21"
metrics-exporter-prometheus = "0.14"
ort = { version = "2.0.0-rc.9", optional = true, default-features = false, features = ["download-binaries"] }
tokenizers = { version = "0.15", optional = true }
ndarray = { version = "0.15", optional = true }
base64 = "0.21"

[features]
default = []
llama = ["dep:llama_cpp"]
onnx = ["dep:ort"]
onnx_tokenizer = ["onnx", "dep:tokenizers", "dep:ndarray"]
llava = ["llama", "onnx"]

